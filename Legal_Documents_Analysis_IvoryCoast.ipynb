{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Legal_Documents_Analysis_IvoryCoast.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus.reader.api import CorpusReader\n",
        "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
        "import nltk.data\n",
        "from nltk.corpus.reader.api import *\n",
        "from nltk.corpus.reader.util import *\n",
        "from nltk.tokenize import *\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from itertools import groupby\n",
        "from operator import itemgetter\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import gensim \n",
        "import gensim.corpora as corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "import pickle\n",
        "import re \n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "SR0X6r-PSGtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_alpha = [] #filter to remove single alphabets\n",
        "for i in nltk.corpus.words.words():\n",
        "  if len(i)==1 :\n",
        "    single_alpha.append(i)\n",
        "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
        "STOPWORDS.update(single_alpha) #list of stopwords including single alphabets"
      ],
      "metadata": {
        "id": "6ytjmnsTXIFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlaintextCorpusReader(CorpusReader): #reader to read texts in .txt format\n",
        "    CorpusView = StreamBackedCorpusView\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        fileids,\n",
        "        word_tokenizer=WordPunctTokenizer(),\n",
        "        sent_tokenizer=nltk.data.LazyLoader(\"tokenizers/punkt/english.pickle\"),\n",
        "        para_block_reader=read_blankline_block,\n",
        "        encoding=\"utf8\"):\n",
        "        CorpusReader.__init__(self, root, fileids, encoding)\n",
        "        self._word_tokenizer = word_tokenizer\n",
        "        self._sent_tokenizer = sent_tokenizer\n",
        "        self._para_block_reader = para_block_reader\n",
        "\n",
        "\n",
        "    def words(self, fileids=None): #word matrix\n",
        "        return concat(\n",
        "            [\n",
        "                self.CorpusView(path, self._read_word_block, encoding=enc)\n",
        "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
        "            ]\n",
        "        )\n",
        "    \n",
        "    def sents(self, fileids=None): #sentence matrix\n",
        "        if self._sent_tokenizer is None:\n",
        "            raise ValueError(\"No sentence tokenizer for this corpus\")\n",
        "\n",
        "        return concat(\n",
        "            [\n",
        "                self.CorpusView(path, self._read_sent_block, encoding=enc)\n",
        "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
        "            ]\n",
        "        )\n",
        "    \n",
        "    def paras(self, fileids=None): #paragraph matrix\n",
        "        if self._sent_tokenizer is None:\n",
        "            raise ValueError(\"No sentence tokenizer for this corpus\")\n",
        "\n",
        "        return concat(\n",
        "            [\n",
        "                self.CorpusView(path, self._read_para_block, encoding=enc)\n",
        "                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def _read_word_block(self, stream):\n",
        "        words = []\n",
        "        for i in range(20):  # Read 20 lines at a time.\n",
        "            words.extend(self._word_tokenizer.tokenize(stream.readline()))\n",
        "        for i in words.copy():\n",
        "          if i.isdigit() and len(i) < 4:\n",
        "            words.remove(i)\n",
        "        return words\n",
        "\n",
        "    def _read_sent_block(self, stream):\n",
        "        sents = []\n",
        "        for para in self._para_block_reader(stream):\n",
        "            sents.extend(\n",
        "                [\n",
        "                    self._word_tokenizer.tokenize(sent)\n",
        "                    for sent in self._sent_tokenizer.tokenize(para)\n",
        "                ]\n",
        "            )\n",
        "        return sents\n",
        "\n",
        "    def _read_para_block(self, stream):\n",
        "        paras = []\n",
        "        for para in self._para_block_reader(stream):\n",
        "            paras.append(\n",
        "                [\n",
        "                    self._word_tokenizer.tokenize(sent)\n",
        "                    for sent in self._sent_tokenizer.tokenize(para)\n",
        "                ]\n",
        "            )\n",
        "        return paras\n",
        "\n",
        "from nltk.cluster import KMeansClusterer\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "def is_punct(token): #is every character a punctuation?\n",
        "    return all(\n",
        "        unicodedata.category(char).startswith('P')\n",
        "        for char in token\n",
        "    )\n",
        "\n",
        "\n",
        "def wnpos(tag): #tags by parts of speech\n",
        "    return {\n",
        "        'N': wn.NOUN,\n",
        "        'V': wn.VERB,\n",
        "        'R': wn.ADV,\n",
        "        'J': wn.ADJ\n",
        "    }.get(tag[0], wn.NOUN)\n",
        "\n",
        "\n",
        "def normalize(document, stopwords=STOPWORDS): #normalize (clean) the documents\n",
        "    for token in document:\n",
        "        token = token.lower().strip()\n",
        "        if is_punct(token) or (token in stopwords):\n",
        "            continue\n",
        "\n",
        "        yield lemmatizer.lemmatize(token)\n",
        "\n",
        "\n",
        "class KMeansTopics(object):\n",
        "    def __init__(self, corpus, k=10):\n",
        "        self.k = k\n",
        "        self.model = None\n",
        "        self.vocab = list(\n",
        "            set(normalize(corpus.words()))\n",
        "            )\n",
        "\n",
        "    def vectorize(self, document): #vectorize the contents of documents using one-hot encoding\n",
        "        features = set(normalize(document))\n",
        "        return np.array([\n",
        "            token in features for token in self.vocab], np.short)\n",
        "\n",
        "    def cluster(self, corpus): #k-means clustering\n",
        "        cosine = nltk.cluster.util.cosine_distance\n",
        "        self.model = KMeansClusterer(\n",
        "            self.k, distance=cosine, avoid_empty_clusters=True)\n",
        "        self.model.cluster([\n",
        "            self.vectorize(\n",
        "                corpus.words(fileid)\n",
        "            ) for fileid in corpus.fileids()\n",
        "        ])\n",
        "\n",
        "    def classify(self, document):\n",
        "        return self.model.classify(self.vectorize(document))\n",
        "\n",
        "IvoryCoast_corpus = PlaintextCorpusReader('Ivory Coast', fileids='.*\\.txt')\n",
        "IvoryCoast_words  = Counter(IvoryCoast_corpus.words())\n",
        "clusterer = KMeansTopics(IvoryCoast_corpus, k=4)\n",
        "clusterer.cluster(IvoryCoast_corpus)\n",
        "\n",
        "groups = [\n",
        "        (clusterer.classify(IvoryCoast_corpus.words(fileid)), fileid)\n",
        "        for fileid in IvoryCoast_corpus.fileids()\n",
        "    ]\n",
        "\n",
        "groups.sort(key=itemgetter(0)) #clustering of corpus documents\n",
        "for group, items in groupby(groups, key=itemgetter(0)):\n",
        "  for cluster, fname in items:\n",
        "    print(\"Cluster {}: {}\".format(cluster+1,fname))"
      ],
      "metadata": {
        "id": "Qmx4UGj9rN9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalTopics(object): #dendrogram\n",
        "    def __init__(self, corpus):\n",
        "        self.model = None\n",
        "        self.vocab = list(\n",
        "            set(normalize(corpus.words()))\n",
        "        )\n",
        "\n",
        "    def vectorize(self, document):\n",
        "        features = set(normalize(document))\n",
        "        return np.array([\n",
        "            token in features for token in self.vocab], np.short)\n",
        "\n",
        "    def cluster(self, corpus):\n",
        "        self.model = AgglomerativeClustering()\n",
        "        self.model.fit_predict([\n",
        "            self.vectorize(\n",
        "                corpus.words(fileid)) for fileid in\n",
        "            corpus.fileids(\n",
        "                           )\n",
        "        ])\n",
        "\n",
        "        self.labels = self.model.labels_\n",
        "        self.children = self.model.children_\n",
        "\n",
        "    def plot_dendrogram(self, **kwargs): \n",
        "        distance = np.arange(self.children.shape[0]) #distances between each pair of children\n",
        "        position = np.arange(self.children.shape[0])\n",
        "\n",
        "        linkage_matrix = np.column_stack([\n",
        "            self.children, distance, position]\n",
        "        ).astype(float) #linkage matrix for children\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(15, 7))  \n",
        "        ax = dendrogram(linkage_matrix, **kwargs)\n",
        "        plt.rcParams['font.size'] = '16'\n",
        "        plt.tick_params(axis='x', bottom='off', top='off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from collections import Counter\n",
        "    corpus = IvoryCoast_corpus\n",
        "    labels = []\n",
        "    for fileid in corpus.fileids():\n",
        "        terms = []\n",
        "        for term, count in Counter(list(normalize(corpus.words(fileid)))).most_common(10):\n",
        "            terms.append(term)\n",
        "        labels.append(terms)\n",
        "\n",
        "    clusterer = HierarchicalTopics(corpus)\n",
        "    clusterer.cluster(corpus)\n",
        "    clusterer.plot_dendrogram(leaf_font_size=16)"
      ],
      "metadata": {
        "id": "Nt7RSs9FYj6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(IvoryCoast.sents())\n",
        "for sents in data:\n",
        "    for token in sents.copy():\n",
        "        token = token.lower().strip()\n",
        "        if (token in sents) and (is_punct(token) or (token in STOPWORDS) or (token.isdigit() and len(token) < 4)):\n",
        "          sents.remove(token)\n",
        "id2word = Dictionary(data)\n",
        "corpus = [id2word.doc2bow(text) for text in data] #dictionarized data for lda\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                   id2word=id2word,\n",
        "                   num_topics=10, \n",
        "                   random_state=0,\n",
        "                   chunksize=100,\n",
        "                   alpha='auto',\n",
        "                   per_word_topics=True) #latent dirichlet analysis\n",
        "\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "1b7nDno1cCG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "p=gensimvis.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.save_html(p, 'lda.html')"
      ],
      "metadata": {
        "id": "XH8yunih79YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "id": "zf8mrcYFof7B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}